{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1-2NGnBscJo"
   },
   "source": [
    "## 4. Data Ops - Deploy Spark pipeline using Dataproc Workflows\n",
    "\n",
    "### Dataproc Workflows\n",
    "\n",
    "Dataproc Workflows has 2 types of workflow templates.\n",
    "\n",
    "1. Manged cluster - Create a new cluster and delete the cluster once the job has completed.\n",
    "2. Cluster selector - Select a pre-existing Dataproc cluster to the run the jobs (does not delete the cluster).\n",
    "\n",
    "This module will use option 1 to create a managed cluster workflow template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYHGVNfTscJo"
   },
   "source": [
    "### 4.1 Write pyspark jobs for dataproc workflow\n",
    "\n",
    "* Job 1: To convert CSV to BQ Tables\n",
    "* Job 2: Run predictions on trained model and persist results\n",
    "\n",
    "Resources:\n",
    "* Learn more about dataproc workflows [[here]](https://cloud.google.com/dataproc/docs/concepts/workflows/overview)\n",
    "* Learn more about adding pyspark jobs to a worfklow template [[here]](https://cloud.google.com/sdk/gcloud/reference/dataproc/workflow-templates/add-job/pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup BQ tables for persisiting results from pyspark jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create bq tables to persist the results of the jobs\n",
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null \n",
    "project_id = project_id[0]\n",
    "# Table definition for both jobs \n",
    "job1_table_name = project_id + '-raw.transaction_data_workflow'\n",
    "job2_table_name = project_id + '-annotated.transaction_data_workflow'\n",
    "job1_table_name = job1_table_name.replace('-', '_')\n",
    "job2_table_name = job2_table_name.replace('-', '_')\n",
    "# Schema definition for tables\n",
    "schema_inline = \"type:string,amount:float,oldbalanceOrg:float,newbalanceOrig:float,isFraud:integer,transactionID:string,prediction:integer\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq mk --table \\\n",
    "{job1_table_name} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq mk --table \\\n",
    "{job2_table_name} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Job 1 (Read csv and persist in BQ) \n",
    "\n",
    "**TODO**\n",
    "* Provide project_id and train_data_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCQwAUOfscJp"
   },
   "outputs": [],
   "source": [
    "%%writefile job_csv_to_bq_table.py\n",
    "## Job 1\n",
    "print('Job 1')\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Automated Data Engineer Workflow') \\\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    ".config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# variables\n",
    "project_id = '<insert-code-here>'\n",
    "gcs_bucket = project_id + '-data'\n",
    "train_data_path = '<insert-code-here>'\n",
    "table_name = project_id + '-raw.transaction_data_workflow'\n",
    "table_name = table_name.replace('-', '_')\n",
    "\n",
    "df_transaction_data_from_csv = spark \\\n",
    "  .read \\\n",
    "  .option (\"inferSchema\" , \"true\") \\\n",
    "  .option (\"header\" , \"true\") \\\n",
    "  .csv (train_data_path)\n",
    "\n",
    "df_transaction_data_from_csv.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", table_name) \\\n",
    ".option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Job 2 (Run predictions on trained model and persist results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **TODO** (Challenge 1)\n",
    "* Provide project_id and train_data_path variables \n",
    "* Provide code to: \n",
    "    * load test data into a pyspark dataframe \n",
    "    * load previously trained ML model \n",
    "    * run prediction on test data \n",
    "    * persist relevant fields into BQ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job_ml_predictions.py\n",
    "print('Job 2')\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "path_to_predict_csv = '<insert-code-here>'\n",
    "project_id = '<insert-code-here>'\n",
    "gcs_bucket = project_id + '-data'\n",
    "model_path = 'gs://' + gcs_bucket + '/model/'\n",
    "table_name = project_id + '-annotated.transaction_data_workflow'\n",
    "table_name = table_name.replace('-', '_')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Automated Data Scientist Workflow') \\\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    ".config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\") \\\n",
    ".getOrCreate()\n",
    "\n",
    "# Load the test data into a spark dataframe \n",
    "df_test_data = <insert-code-here>\n",
    "# Load the previously trained pipeline model \n",
    "loaded_pipeline_model = <insert-code-here>\n",
    "# Run predictions on the test data\n",
    "predictions = <insert-code-here>\n",
    "\n",
    "# Save the following fields into BQ table: 'type', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'isFraud', 'transactionID', 'prediction' \n",
    "# Use the bq table name defined previously {table_name}\n",
    "<insert-code-here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2fL7Z5iscJr"
   },
   "source": [
    "### 4.2 Grant additional service account permission to deploy workflow from notebooks\n",
    "\n",
    "Go to https://console.cloud.google.com/iam-admin/iam\n",
    "\n",
    "Look for Compute Engine default service account. It will be in the format\n",
    "\n",
    "```\n",
    "{number}-compute@developer.gserviceaccount.com\n",
    "```\n",
    "\n",
    "Edit the roles and add the role \"Storage Object Admin\" and press save.\n",
    "\n",
    "Note that these steps are taken to deploy the workflow directly from the notebooks. Alternatively you can execute the steps below via cloud shell. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC7qXjdMscJt"
   },
   "source": [
    "### 4.3 Create Dataproc managed cluster workflow Template\n",
    "\n",
    "Learn more about creating workflow templates [[here]](https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows#creating_a_template)\n",
    "\n",
    "**TODO**\n",
    "* Provide values for variables below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env WORKFLOW_ID=automate-bank-transaction-workflow\n",
    "%env REGION=<insert-code-here>\n",
    "%env PROJECT_ID=<insert-code-here>\n",
    "%env BUCKET_NAME=<insert-code-here>\n",
    "%env CLUSTER_NAME=spark-workflow-cluster\n",
    "%env NUM_WORKERS=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUQ7vcnuscJw"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud dataproc workflow-templates create $WORKFLOW_ID \\\n",
    "--region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8BqGX5VscJw"
   },
   "source": [
    "### 4.4 Configure managed cluster for the workflow template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVl2u2mSscJx"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud beta dataproc workflow-templates set-managed-cluster $WORKFLOW_ID \\\n",
    "    --cluster-name $CLUSTER_NAME \\\n",
    "    --region $REGION \\\n",
    "    --image-version=1.5-ubuntu18 \\\n",
    "    --master-machine-type n1-standard-2 \\\n",
    "    --master-boot-disk-size=128GB \\\n",
    "    --num-workers $NUM_WORKERS \\\n",
    "    --worker-machine-type n1-standard-2\\\n",
    "    --worker-boot-disk-size=128GB \\\n",
    "    --scopes https://www.googleapis.com/auth/cloud-platform \\\n",
    "    --bucket $BUCKET_NAME \\\n",
    "    --properties spark:spark.jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.18.0.jar,spark:spark=gs://spark-lib/bigquery/spark-bigquery-latest.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZG53bc48scJy"
   },
   "source": [
    "### 4.5 Upload PySpark job to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O0wO8H7scJz"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp job_csv_to_bq_table.py \\\n",
    " gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_ppc1GfscJz"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil cp job_ml_predictions.py \\\n",
    " gs://${PROJECT_ID}-data/workflows/python-scripts/job_ml_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DX87wdpscJ0"
   },
   "source": [
    "### 4.6 Add job to workflow template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lukmzJbMscJ0"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud dataproc workflow-templates add-job pyspark \\\n",
    "   gs://${PROJECT_ID}-data/workflows/python-scripts/job_csv_to_bq_table.py \\\n",
    "    --region $REGION \\\n",
    "    --step-id csv_to_bq \\\n",
    "    --workflow-template $WORKFLOW_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdtd-QZpscJ1"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud dataproc workflow-templates add-job pyspark \\\n",
    "  gs://${PROJECT_ID}-data/workflows/python-scripts/job_ml_predictions.py \\\n",
    "    --region $REGION \\\n",
    "    --start-after=csv_to_bq \\\n",
    "    --step-id predict \\\n",
    "    --workflow-template $WORKFLOW_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ug2dKqWscJ2"
   },
   "source": [
    "### 4.7 Run workflow template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5UY8epDscJ2",
    "outputId": "5d09224f-4790-4ece-f689-23d604017baf"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud dataproc workflow-templates instantiate $WORKFLOW_ID \\\n",
    "--region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owS3gXdHscJ4"
   },
   "source": [
    "### View Cluster, workflow and jobs tabs\n",
    "\n",
    "Go to the Dataproc UI and view the cluster page. You should see the new cluster spinning up\n",
    "\n",
    "Once the cluster is ready view the workflow and jobs tabs to check the progress of the jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDhN2ph6scJ6"
   },
   "source": [
    "### 4.8 Schedule the workflows\n",
    "**TODO** (Optional: Challenge 2)\n",
    "* Cloud Composer is a managed Apache Airflow service you can use to create, schedule, monitor, and manage workflows.\n",
    "* View the guide on how to schedule Dataproc workflows in composer [[here]](https://cloud.google.com/dataproc/docs/tutorials/workflow-composer)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Spark - Bank Marketing Demo (1).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
