{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SPMMsyxRTcvM"
   },
   "source": [
    "## 1. Data Engineering - Process CSV files into BQ Tables via Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JlWBMwnTcvN"
   },
   "source": [
    "### Create Spark session with BQ connector\n",
    "\n",
    "Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwScF-NUTcvN"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, IntegerType, StructField, StructType\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Spark - Data Eng Demo') \\\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**\n",
    "* Provide your bucket path below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil ls <bucket-path>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIDDxoNETcvN"
   },
   "source": [
    "Check the first 1000 bytes of a file on GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cat -h -r 0-1000 <bucket-path>/transaction_data_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_train_csv = \"<bucket-path>/transaction_data_train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNs5PNQsTcvT"
   },
   "source": [
    "### Get Spark application ID \n",
    "\n",
    "This is useful to easily fine application in the Spark History UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.app.id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZNGS52lTcvW"
   },
   "source": [
    "Load the CSV file into a Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgsapsiMTcvW",
    "outputId": "8f387f4f-9444-406b-d96f-4ba98a0fa606"
   },
   "outputs": [],
   "source": [
    "df_transaction_data_from_csv = spark \\\n",
    ".read \\\n",
    ".option(\"inferSchema\" , \"true\") \\\n",
    ".option(\"header\" , \"true\") \\\n",
    ".csv(path_to_train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the BQ dataset & table to persist data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** (Challenge 1)\n",
    "* Create the BQ schema from the spark dataframe \n",
    "    * Reference for converting data types: https://github.com/GoogleCloudDataproc/spark-bigquery-connector#data-types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_inline = <insert-code-here>\n",
    "schema_inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_from_csv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the name your BQ dataset\n",
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null \n",
    "dataset_name = project_id[0] + '-raw'\n",
    "dataset_name = dataset_name.replace('-', '_')\n",
    "dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the BQ dataset by specifying the location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq --location=europe-west3 mk -d \\\n",
    "{dataset_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path to new table for creation\n",
    "bq_table_name = 'transaction_data_train'\n",
    "bq_table_path= dataset_name + '.' + bq_table_name\n",
    "bq_table_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the BQ table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq mk --table \\\n",
    "{bq_table_path} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIjPZm4gTcvf"
   },
   "source": [
    "#### Check that table was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YA-s2gntTcvf",
    "outputId": "ea505e9f-d857-4162-fffa-1bdf95293e66"
   },
   "outputs": [],
   "source": [
    "table = project_id[0] + \":\" + bq_table_path\n",
    "df_transaction_data_from_bq = spark.read \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", table) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_from_bq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_from_bq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write spark dataframe to BQ table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temp GCS bucket for writing spark df to bq table\n",
    "gcs_bucket = project_id[0] + '-data'\n",
    "gcs_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQ9TGCYiTcvf",
    "outputId": "ad0e5d3b-b06e-4d76-b619-9b7eb971dd2b"
   },
   "outputs": [],
   "source": [
    "df_transaction_data_from_csv.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", table) \\\n",
    ".option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the BQ table is populated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_from_bq = spark.read \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", table) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_from_bq.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGcW4WpJTcvg"
   },
   "source": [
    "### Compute statistics for columns in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transaction_data_from_bq.describe().show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "ZNs5PNQsTcvT",
    "JGcW4WpJTcvg"
   ],
   "name": "test-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
