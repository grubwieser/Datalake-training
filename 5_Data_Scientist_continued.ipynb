{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3TwAK_8Wll0"
   },
   "source": [
    "## 5. Data Scientist - Continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwCbd6SqWll0"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    ".appName('Spark - Data Scientist Demo') \\\n",
    ".config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    ".config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.18.0\") \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.app.id\")\n",
    "spark.sparkContext._jvm.scala.util.Properties.versionString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refactor Table - add a date (day) field\n",
    "Transform the step-column [1,742] to days [1,31]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "bq_enriched_dataset_name = project_id[0] + '-enriched'\n",
    "bq_enriched_dataset_name = bq_enriched_dataset_name.replace('-', '_')\n",
    "bq_enriched_table_path = project_id[0] + ':' + bq_enriched_dataset_name + '.transaction_analysis_enriched' \n",
    "bq_enriched_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", bq_enriched_table_path) \\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('type_OHE','features','rawPrediction','probability')\n",
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = data.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** (Challenge 3)\n",
    "* Convert the column steps into days\n",
    "* Each step corresponds to one hour. The dataset was created over the span of a month. There are 742 steps which should be converted to 31 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df['days'] = <enter-code-here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are all days - this should output day 1 to 31\n",
    "sorted(pd.unique(pandas_df['days']).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df['days'] = pandas_df['days'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some further analyis including visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pandas_df['type'].value_counts().plot(kind='bar',title=\"Number per Typ\")\n",
    "ax.set_xlabel(\"Transaction Type\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pandas_df['days'].value_counts().plot(kind='bar',title=\"Busy day in a month\")\n",
    "ax.set_xlabel(\"Day\")\n",
    "ax.set_ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df.groupby(['type']).sum().plot(kind='pie', y='amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store table in a performance opimized way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_optimized_table_name = 'transaction_data_optimized'\n",
    "bq_optimized_table_path=  project_id[0] +  '_enriched.' + bq_optimized_table_name\n",
    "bq_optimized_table_path = bq_optimized_table_path.replace('-', '_')\n",
    "bq_optimized_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_inline = df.schema.simpleString().replace('struct<', '').replace('>', '').replace('int', 'int64').replace('double', 'float64').replace('bigint64', 'int64').replace('vector', 'STRING').replace('bigint', 'int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bq mk  \\\n",
    "--range_partitioning=days,1,31,1 \\\n",
    "--clustering_fields=days \\\n",
    "{bq_optimized_table_path} \\\n",
    "{schema_inline}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write \\\n",
    ".format(\"bigquery\") \\\n",
    ".option(\"table\", project_id[0]  + ':' + bq_optimized_table_path) \\\n",
    ".option(\"temporaryGcsBucket\", project_id[0]  + '-data') \\\n",
    ".mode('overwrite') \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measure performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_partition_table_path = project_id[0] + ':' + bq_enriched_dataset_name + '.transaction_data_optimized' \n",
    "bq_partition_table_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data in filter by partition\n",
    "partitionset = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\", bq_partition_table_path) \\\n",
    "  .option(\"filter\", 'days >= 5 AND days < 25') \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitionset = partitionset.select(\"days\", \"amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_optimized = partitionset.groupBy('days').agg(F.sum('amount').alias('total_amount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 10\n",
    "aggregation_optimized.orderBy('total_amount', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "test-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
